# AI Talent Engine — Evaluation Based on Determinant Skills (v3.3)

**Author:** L. David Mendoza  
**System:** AI Talent Engine — Research-First Sourcer Automation  
**Schema Reference:** AI_Talent_Schema_Rules v3.2  
**Classification:** Internal Evaluation Framework  
**Last Updated:** December 2025  

---

## Introduction  

The **AI Talent Engine — Evaluation Based on Determinant Skills** framework defines a structured, evidence-driven methodology for evaluating AI professionals based on **verifiable technical contribution**, not inferred capability, résumé claims, or keyword proximity.

Unlike traditional recruiting or résumé-screening approaches, this framework evaluates **primary technical artifacts** — including peer-reviewed publications, open-source repositories, model releases, patents, and conference participation — to assess authentic contribution, technical depth, and projected impact.

The framework is designed to support **high-stakes AI hiring decisions** across Frontier / Foundational AI, RLHF & Alignment, Applied AI, and AI Infrastructure roles by producing **audit-ready, explainable, and repeatable evaluations** grounded entirely in traceable evidence.

---

## Candidate Overview

| Field | Details |
|:-------|:---------|
| Full Name | |
| Current Employer / Affiliation | |
| Primary Area | |
| Evaluation Date | |
| Evaluator | |

---

## Key Determinant Skill Signals  

| Dimension | Description | Evidence Source | Weighted Score |
|:-----------|:-------------|:----------------|:----------------|
| Research Publication Impact | Peer-reviewed contributions in AI safety, ML systems, or foundational models | Semantic Scholar, ArXiv, ACL Anthology | |
| Code Contribution Authenticity | Commit frequency, author identity consistency, and technical complexity | GitHub / GitLab | |
| Model Innovation | Novel architectures or RLHF/Alignment contributions | Papers with Code, HuggingFace | |
| Open Evaluation Replication | Reproducible benchmarks or released evaluation frameworks | Public repos / benchmark records | |
| Cross-Domain Collaboration | Contributions across AI/ML, infra, or applied productization | Co-authorship and repo networks | |
| Institutional Alignment | Alignment with RLHF, safety, and interpretability research standards | Org / research match | |

---

## Scoring Summary  

| Level | Range | Interpretation |
|:------|:-------|:----------------|
| **Frontier (9–10)** | Breakthrough or leadership-level contributions (e.g., alignment research, model architecture design). |
| **Foundational (7–8)** | Stable, independent research or major applied infrastructure work. |
| **Applied (5–6)** | Strong technical contribution with measurable outcomes in ML systems or production AI. |
| **Emerging (3–4)** | Demonstrated academic or experimental involvement; limited verified production output. |
| **Preliminary (1–2)** | Exploratory contributions or unverifiable claims. |

---

## Recruiter Communication Template  

**Subject:** Determinant Skills Evaluation — [Candidate Name]  
**Body:**  
> Following an independent determinant-based evaluation, [Candidate Name] demonstrates verifiable contributions in [specific domain].  
> Evaluation Score: [X.X / 10]  
> Evidence sources include: [Top 3 artifacts].  
> Recommended next action: [Shortlist / Technical Deep Dive / Hold].  

---

## Governance & Audit  

All evaluations are stored under `schema_v3.3` and linked to immutable commit references for audit integrity.  
Versioning ensures that all evaluation updates remain schema-safe and verifiable against the AI Talent Engine framework.
